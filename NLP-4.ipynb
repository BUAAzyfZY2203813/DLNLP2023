{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def is_uchar(uchar):\n",
    "    \"\"\"判断一个字符是否是汉字、数字、英文字母或特定符号\"\"\"\n",
    "    if '\\u4e00' <= uchar <= '\\u9fa5':  # 汉字范围\n",
    "        return True\n",
    "    if '\\u0030' <= uchar <= '\\u0039':  # 数字范围\n",
    "        return True\n",
    "    if ('\\u0041' <= uchar <= '\\u005a') or ('\\u0061' <= uchar <= '\\u007a'):  # 英文字母范围\n",
    "        return True\n",
    "    if uchar in '，。：？“”！；、《》——':  # 特定符号\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNNModel:\n",
    "    def __init__(self, BATCH_SIZE, HIDDEN_SIZE, HIDDEN_LAYERS, VOCAB_SIZE, learning_rate):\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.HIDDEN_SIZE = HIDDEN_SIZE\n",
    "        self.HIDDEN_LAYERS = HIDDEN_LAYERS\n",
    "        self.VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "        # 定义占位符\n",
    "        self.inputs = tf.placeholder(tf.int32, [BATCH_SIZE, None])\n",
    "        self.targets = tf.placeholder(tf.int32, [BATCH_SIZE, None])\n",
    "        self.keepprb = tf.placeholder(tf.float32)\n",
    "\n",
    "        # 定义词嵌入层\n",
    "        embedding = tf.get_variable('embedding', [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        emb_input = tf.nn.embedding_lookup(embedding, self.inputs)\n",
    "        emb_input = tf.nn.dropout(emb_input, self.keepprb)\n",
    "\n",
    "        # 搭建LSTM结构\n",
    "        lstm = tf.contrib.rnn.LSTMCell(HIDDEN_SIZE, state_is_tuple=True)\n",
    "        lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=self.keepprb)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm] * HIDDEN_LAYERS)\n",
    "        self.initial_state = cell.zero_state(BATCH_SIZE, tf.float32)\n",
    "        outputs, self.final_state = tf.nn.dynamic_rnn(cell, emb_input, initial_state=self.initial_state)\n",
    "\n",
    "        # 重新reshape输出\n",
    "        outputs = tf.reshape(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "        w = tf.get_variable('outputs_weight', [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "        b = tf.get_variable('outputs_bias', [VOCAB_SIZE])\n",
    "        logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "        # 计算损失\n",
    "        self.loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self.targets, [-1])],\n",
    "                                                                       [tf.ones([BATCH_SIZE * TIME_STEPS],\n",
    "                                                                                dtype=tf.float32)])\n",
    "        self.cost = tf.reduce_sum(self.loss) / BATCH_SIZE\n",
    "\n",
    "        # 优化算法\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(learning_rate, global_step, BATCH_NUMS, 0.99, staircase=True)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(grads, trainable_variables))\n",
    "\n",
    "        # 预测输出\n",
    "        self.predict = tf.argmax(logits, 1)\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "with open('./data/白马啸西风.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = re.sub(r'\\(.*\\)', '', data)  # 删除括号及其中内容\n",
    "data = data.replace('……', '。')  # 替换省略号为句号\n",
    "\n",
    "vocab = set(data)\n",
    "id2char = list(vocab)\n",
    "char2id = {c: i for i, c in enumerate(vocab)}\n",
    "\n",
    "word_data = list(jieba.cut(data))\n",
    "word_vocab = set(word_data)\n",
    "id2word = list(word_vocab)\n",
    "word2id = {w: i for i, w in enumerate(word_vocab)}\n",
    "\n",
    "numdata = np.array([char2id[char] for char in data])\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 8\n",
    "TIME_STEPS = 100\n",
    "BATCH_NUMS = len(numdata) // (BATCH_SIZE * TIME_STEPS)\n",
    "HIDDEN_SIZE = 512\n",
    "HIDDEN_LAYERS = 6\n",
    "MAX_GRAD_NORM = 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "model = RNNModel(BATCH_SIZE, HIDDEN_SIZE, HIDDEN_LAYERS, VOCAB_SIZE, learning_rate)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('logs/tensorboard', tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for k in range(EPOCHS):\n",
    "        state = sess.run(model.initial_state)\n",
    "        train_data = data_generator(numdata, BATCH_SIZE, TIME_STEPS)\n",
    "        total_loss = 0.\n",
    "        for i in range(BATCH_NUMS):\n",
    "            xs, ys = next(train_data)\n",
    "            feed = {model.inputs: xs, model.targets: ys, model.keepprb: 0.8, model.initial_state: state}\n",
    "            costs, state, _ = sess.run([model.cost, model.final_state, model.opt], feed_dict=feed)\n",
    "            total_loss += costs\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print('epochs:', k + 1, 'iter:', i + 1, 'cost:', total_loss / (i + 1))\n",
    "    saver.save(sess, './checkpoints/lstm.ckpt')\n",
    "\n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之死于参战逃向针阵，莫铁桶分布之性即解众山静各地当世已回头是岸首第一席抄录提此议二十多年已先绞成可信通候妆次煤灰切削借来编好领头人所敢转入郭襄制一拉一东躲西寿礼难以相信恰容心境银杏树指挥绵掌不辞，一大说五姑决不再抢入\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "evalmodel = RNNModel(1, HIDDEN_SIZE, HIDDEN_LAYERS, VOCAB_SIZE, learning_rate)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './checkpoints/lstm.ckpt')\n",
    "    new_state = sess.run(evalmodel.initial_state)\n",
    "    T = []\n",
    "    with open('./data/白马啸西风_target.txt', 'r', encoding='utf-8') as f:\n",
    "        T = [char2id[n.strip()] for n in f]\n",
    "\n",
    "    x = np.array([T])\n",
    "    samples = []\n",
    "    for i in range(100):\n",
    "        feed = {evalmodel.inputs: x, evalmodel.keepprb: 1., evalmodel.initial_state: new_state}\n",
    "        c, new_state = sess.run([evalmodel.predict, evalmodel.final_state], feed_dict=feed)\n",
    "        x[0] = c\n",
    "        samples.append(c[0])\n",
    "\n",
    "    print('test:', ''.join([id2char[index] for index in samples]))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
