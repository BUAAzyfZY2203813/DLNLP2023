{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成的噪声数据替换点： 21276696\n",
      "替换的噪声符号：\n",
      "〖 \n",
      "〗 — ｉ ｏ ｇ ｅ ｎ ＋ ｄ · ｓ ｃ ｂ ａ ｒ ｊ \u001A ｕ ～ ｈ ｔ － ｌ ─ ＜ ＞ ｐ ３ ｋ Ｖ ＠ ○ ９ １ 〓 ① ② ③ Ｈ Ｃ Ｔ ④ ⑤ ｆ Ａ ｙ Ｓ ● ⑥ ⑧ \n",
      "Ｇ ｍ ４ ⑦    ┦ Ｐ ∶  Ｋ ＂    × ５ ※ ６ ⑨ Ｍ  Ｂ  ８ ｗ Ｌ ２ ０ Ｒ        ［ ］   Ｗ ｚ ｘ  ⒑    \n",
      "          Γ  ＝ ⒅   Ｄ τ  ⑹ Ｙ Ｊ Ｅ Ｆ Ｎ ｖ  ⒌    ǖ ァ    Ｏ      〕   ７ δ   ソ \n",
      "     χ 〈 α ィ    Φ  ば  ⑼ ￡ ο π С   ￣ ／  ┳ チ   ɑ シ ┕  ㄅ  ┒ б  ┰        Щ γ  \n",
      "   ド ≌   ζ  ち  ∑    ξ  ∝  ┨   ń      ┬      フ     ρ ＃  Α   ┢ ㈡   Ρ \n",
      " 〉 セ υ ぶ            ビ    ㄍ  ゲ        Τ ∽ ぷ     ㄊ  Ω           \n",
      "  ⒍   ぃ   ジ  Ψ  ⑸         ∪  ⑾    σ ⑷  て  ┯   Ｘ Ｚ ⒃ ∩    é Ｉ   ＼  Υ ℃ \n",
      "    Ｑ "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def get_filename(path):\n",
    "    name = []\n",
    "    file_name = os.listdir(path)\n",
    "    for novel_name in file_name:\n",
    "        name.append(path + '//' + novel_name)\n",
    "    return name\n",
    "\n",
    "name_list = get_filename(r'./data')\n",
    "\n",
    "words = []\n",
    "for path in name_list:\n",
    "    with open(path, \"r\", encoding=\"ANSI\") as file:\n",
    "        line = [line.strip(\"\\n\").replace(\"\\u3000\", \"\").replace(\"\\t\", \"\") for line in file][3:]#去掉换行，替换空格\n",
    "        words += line\n",
    "# words 存储语料库，其中以每一个自然段为一个分割\n",
    "regex_str = \".*?([^\\u4E00-\\u9FA5]).*?\"#正则表达式：匹配除/n之外的中文字符\n",
    "english = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:：;「<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'#英文字符\n",
    "symbol = []\n",
    "for j in range(len(words)):\n",
    "    words[j] = re.sub(english, \"\", words[j])#将每一段文字中的英文字符替换成空格\n",
    "    symbol += re.findall(regex_str, words[j])#使用正则表达式将摘除英文字符的段落提取出中文字符\n",
    "word_times = Counter(symbol)#统计词频\n",
    "times_top = word_times.most_common()#频率前十的词\n",
    "noise_symbol = []\n",
    "for eve_tuple in times_top:\n",
    "    if eve_tuple[1] < 200:#如果词频在200以下说明是噪声数据\n",
    "        noise_symbol.append(eve_tuple[0])\n",
    "noise_number = 0\n",
    "for line in words:#替换噪声\n",
    "    for noise in noise_symbol:\n",
    "        line.replace(noise, \"\")\n",
    "        noise_number += 1\n",
    "print(\"完成的噪声数据替换点：\", noise_number)\n",
    "print(\"替换的噪声符号：\")\n",
    "for i in range(len(noise_symbol)):\n",
    "    print(noise_symbol[i], end=\" \")\n",
    "    if i % 50 == 0:\n",
    "        print()\n",
    "with open(\"预处理后的文本.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in words:\n",
    "        if len(line) > 1:\n",
    "            print(line, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import math\n",
    "with open(\"预处理后的文本.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = [eve.strip(\"\\n\") for eve in f]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词库总词数： 4314424   不同词的个数： 173004\n",
      "出现频率前10的1-gram词语： [('的', 115604), ('了', 104527), ('他', 64712), ('是', 64457), ('道', 58623), ('我', 57483), ('你', 56679), ('在', 43691), ('也', 32606), ('这', 32199)]\n",
      "entropy_1gram: 12.1667882280937\n"
     ]
    }
   ],
   "source": [
    "# 1-gram\n",
    "token = []\n",
    "for para in words:\n",
    "    token += jieba.lcut(para)#中文分词\n",
    "token_num = len(token)\n",
    "count = Counter(token)\n",
    "vocab1 = count.most_common()\n",
    "entropy_1gram = sum([-(eve[1]/token_num)*math.log((eve[1]/token_num),2) for eve in vocab1])#1-gram算法计算信息熵\n",
    "print(\"总词数：\", token_num, \" \", \"不同词的个数：\", len(vocab1))\n",
    "print(\"出现频率前10的1-gram词语：\", vocab1[:10])\n",
    "print(\"entropy_1gram:\", entropy_1gram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2-gram\n",
    "def combine2gram(cutword_list):\n",
    "    if len(cutword_list) == 1:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(len(cutword_list)-1):\n",
    "        res.append(cutword_list[i] + \"kk\"+ cutword_list[i+1])\n",
    "    return res\n",
    "token_2gram = []\n",
    "for para in words:\n",
    "    cutword_list = jieba.lcut(para)\n",
    "    token_2gram += combine2gram(cutword_list)\n",
    "# 2-gram的频率统计\n",
    "token_2gram_num = len(token_2gram)\n",
    "ct2 = Counter(token_2gram)\n",
    "vocab2 = ct2.most_common()\n",
    "# print(vocab2[:20])\n",
    "# 2-gram相同句首的频率统计\n",
    "same_1st_word = [eve.split(\"kk\")[0] for eve in token_2gram]\n",
    "assert token_2gram_num == len(same_1st_word)\n",
    "ct_1st = Counter(same_1st_word)\n",
    "vocab_1st = dict(ct_1st.most_common())\n",
    "entropy_2gram = 0\n",
    "for eve in vocab2:\n",
    "    p_xy = eve[1]/token_2gram_num\n",
    "    first_word = eve[0].split(\"s\")[0]\n",
    "    # p_y = eve[1]/vocab_1st[first_word]\n",
    "    entropy_2gram += -p_xy*math.log(eve[1]/vocab_1st[first_word], 2)\n",
    "print(\"词库总词数：\", token_2gram_num, \" \", \"不同词的个数：\", len(vocab2))\n",
    "print(\"出现频率前10的2-gram词语：\", vocab2[:10])\n",
    "print(\"entropy_2gram:\", entropy_2gram)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
